\documentclass[12pt]{article}
\usepackage{url,amsmath,setspace,amssymb}
\usepackage{listings}
\usepackage{caption}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{color}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{xassoccnt}

\NewTotalDocumentCounter{totalpages}
\DeclareAssociatedCounters{page}{totalpages}

%**********************************************
%* Leave the page configuration as is
\setlength{\oddsidemargin}{.25in}
\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6.25in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{8.5in}

\renewcommand{\thepage}{\TotalValue{totalpages}-\arabic{page}}
\newcommand{\heading}[5]{
\noindent
\begin{center}
	\framebox[\textwidth]{
	\begin{minipage}{0.9\textwidth} \onehalfspacing
	{\textbf {#1}}

	{\centering \Large #5

	}\medskip
	{#3 \hfill #2 \hfill #4}
	\end{minipage}
}
\end{center}
}

%**********************************************


%**********************************************
%* Some more or less useful stuff

\lstnewenvironment{myalgorithm}[1][]
{
    \lstset{
        mathescape=true,
        frame=none,
        numbers=none,
        basicstyle=\normalsize,
        keywordstyle=\color{black}\bfseries\em,
        keywords={,input, output, return, datatype, function, in, if, else, foreach, while, begin, end, },
        numbers=left,
        xleftmargin=.04\textwidth,
        #1
    }
}
{}

\newtcolorbox{alert}[1]{
colback=red!5!white, colframe=red!75!white,fonttitle=\bfseries, title = #1}

\newtcolorbox{commentbox}[1]{
colback=black!5!white, colframe=black!75!white,fonttitle=\bfseries, title = #1}

\newcommand{\handout}[5]{\heading{#1}{#2}{#3}{#4}{#5}}

\begin{document}

\handout{650.060 - Small Project in Artificial Intelligence and Cybersecurity}{Summer Term 2022/2023}{Francesco Bombassei De Bona}{12138677}{An API for Mutual Information estimation}

    \section{Introduction}\label{sec:intro}
    This project aims to provide a publicly available C++ API for the methodologies introduced in~\cite{roy_leakage_2022} for estimating Mutual Information.
    A section is dedicated to compare the proposed implementation with the histogram estimator method.

    \section{Background and terminology}\label{sec:background}
    \subsection{Mutual Information}\label{subsec:mi}
    Mutual Information (MI) is a measure of the statistical dependence between two random variables.
    In the context of side-channel analysis, MI is used to quantify the dependence between an observed leakage and a secret key.
    Higher values of MI indicate a stronger dependence between the two variables.
    Thus, an higher MI value leads to a higher leakage of information about the secret key by observing a side-channel trace.

    The MI between two random variables $X$ and $Y$ is defined as:
    \begin{align}
        I(X;Y) &= H(X) - H(X|Y) \\ &= H(X) + H(Y) - H(X,Y)\label{eq:mi}
    \end{align}
    where $H(X)$ is the entropy of $X$, $H(X|Y)$ is the conditional entropy of $X$ given $Y$ and is defined as \[H(X|Y) = \begin{cases} \sum_y p(y) H(X|Y=y) & \text{if } Y \text{ is discrete} \\ \int_y p(y) H(X|Y=y) dy & \text{if } Y \text{ is continuous} \end{cases}\] and $H(X,Y)$ is the joint entropy of $X$ and $Y$.

    \subsection{Histogram estimator}\label{subsec:hist}
    The histogram estimator is a method for estimating the MI between two random variables $X$ and $Y$.
    This approach is particularly useful when the underlying probability distributions are not known or difficult to model analytically.
    The core idea behind the histogram estimator is to discretize the continuous random variables into bins and then compute the mutual information based on the joint and marginal frequencies of these bins.
    The MI between $X$ and $Y$ is then estimated as:
    \begin{equation}
        I(X;Y) = H(Y) - H(Y|X)\label{eq:hist}
    \end{equation}
    where $H(Y)$ is the entropy of $Y$ and $H(Y|X)$ is the conditional entropy of $Y$ given $X$.

    \subsection{GKOV estimator}\label{subsec:gkov}
    The GKOV estimator is a method for estimating the MI between two random variables $X$ and $Y$ introduced by Gao, Krishnan, Oh and Vishwanath ~\cite{gkov}.
    This estimator can be used on a combination of discrete and continuous random variables.
    The GKOV estimator is defined as:
    \begin{equation}
        I_n(X;Y) = \frac{1}{n} \sum_{i=1}^n \hat{I}_i = \sum_{i=1}^n \left( \psi(\tilde{t}_i) + \log n - \log(n_{x,i}+1) - \log(n_{y,i}+1) \right)\label{eq:gkov}
    \end{equation}
    where $\psi$ is the digamma function.

    \section{Project Execution}\label{sec:execution}
    The project aimed to develop a C++ API for the GKOV estimator, the histogram estimator, and a simulator for the leakage of a device.
    All the code was written in C++ and using an OO approach.

    \subsection{GKOV estimator}\label{subsec:gkov_impl}
    The GKOV estimator was implemented as described in~\cite{gkov}.
    The implementation is based on the class \texttt{GKOVEstimator} which is initialized with the callback function to compute the value $t_n$ as described in~\cite{roy_leakage_2022}.


    \newpage
    \bibliography{references}
    \bibliographystyle{acm}
\end{document}



