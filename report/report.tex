\documentclass[12pt]{article}
\usepackage{url,amsmath,setspace,amssymb}
\usepackage{listings}
\usepackage{caption}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{color}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{xassoccnt}

\NewTotalDocumentCounter{totalpages}
\DeclareAssociatedCounters{page}{totalpages}

%**********************************************
%* Leave the page configuration as is
\setlength{\oddsidemargin}{.25in}
\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6.25in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{8.5in}

\renewcommand{\thepage}{\TotalValue{totalpages}-\arabic{page}}
\newcommand{\heading}[5]{
    \noindent
    \begin{center}
        \framebox[\textwidth]{
            \begin{minipage}{0.9\textwidth} \onehalfspacing
            {\textbf {#1}}

            {\centering \Large #5

            }\medskip
            {#3 \hfill #2 \hfill #4}
            \end{minipage}
        }
    \end{center}
}

%**********************************************


%**********************************************
%* Some more or less useful stuff

\lstnewenvironment{myalgorithm}[1][]
{
    \lstset{
        mathescape=true,
        frame=none,
        numbers=none,
        basicstyle=\normalsize,
        keywordstyle=\color{black}\bfseries\em,
        keywords={,input, output, return, datatype, function, in, if, else, foreach, while, begin, end, },
        numbers=left,
        xleftmargin=.04\textwidth,
        #1
    }
}
{}

\newtcolorbox{alert}[1]{
    colback=red!5!white, colframe=red!75!white,fonttitle=\bfseries, title = #1}

\newtcolorbox{commentbox}[1]{
    colback=black!5!white, colframe=black!75!white,fonttitle=\bfseries, title = #1}

\newcommand{\handout}[5]{\heading{#1}{#2}{#3}{#4}{#5}}

\begin{document}

    \handout{650.060 - Small Project in Artificial Intelligence and Cybersecurity}{Summer Term 2022/2023}{Francesco Bombassei De Bona}{12138677}{An API for Mutual Information estimation}

    \section{Introduction}\label{sec:intro}
    This project aims to provide a publicly available C++ API for the methodologies introduced in~\cite{roy_leakage_2022} for estimating Mutual Information.
    A section is dedicated to compare the proposed implementation with the histogram estimator method.

    \section{Background and terminology}\label{sec:background}
    \subsection{Mutual Information}\label{subsec:mi}
    Mutual Information (MI) is a measure of the statistical dependence between two random variables.
    In the context of side-channel analysis, MI is used to quantify the dependence between an observed leakage and a secret key.
    Higher values of MI indicate a stronger dependence between the two variables.
    Thus, an higher MI value leads to a higher leakage of information about the secret key by observing a side-channel trace.

    The MI between two random variables $X$ and $Y$ is defined as:
    \begin{align}
        I(X;Y) &= H(X) - H(X|Y) \\ &= H(X) + H(Y) - H(X,Y)\label{eq:mi}
    \end{align}
    where $H(X)$ is the entropy of $X$, $H(X|Y)$ is the conditional entropy of $X$ given $Y$ and is defined as \[H(X|Y) = \begin{cases} \sum_y p(y) H(X|Y=y) & \text{if } Y \text{ is discrete} \\ \int_y p(y) H(X|Y=y) dy & \text{if } Y \text{ is continuous} \end{cases}\] and $H(X,Y)$ is the joint entropy of $X$ and $Y$.

    As presented in~\cite{roy_leakage_2022}, it's possible to observe three different cases of MI:
    \begin{itemize}
        \item $X$ and $Y$ discrete that leads to the discrete MI;
        \item $X$ and $Y$ continuous that leads to the continuous MI;
        \item $X$ discrete and $Y$ continuous that leads to the mixed MI\@.
    \end{itemize}

    In this work, we focus on the mixed MI case.

    For this case there are two different ways to compute the MI between $X$ and $Y$: using an estimator for the conditional/joint density in combination with the H2 or H3 formulas; or using an estimator based on nearest neighbors search.

    \subsection{Histogram estimator}\label{subsec:hist}
    The histogram estimator is a method for estimating the MI between two random variables $X$ and $Y$.
    The core idea behind the histogram estimator is to discretize the continuous random variables into bins and then compute the mutual information based on the joint and marginal frequencies of these bins.
    The MI between $X$ and $Y$ is then estimated using the H2 equation as:
    \begin{equation}
        I(X;Y) = H(Y) - H(Y|X)\label{eq:hist}
    \end{equation}
    where $H(Y)$ is the entropy of $Y$ and $H(Y|X)$ is the conditional entropy of $Y$ given $X$.

    \subsection{GKOV estimator}\label{subsec:gkov}
    The GKOV estimator is a method for estimating the MI between two random variables $X$ and $Y$ introduced by Gao, Krishnan, Oh and Vishwanath ~\cite{gkov}.
    This estimator can be used on a combination of discrete and continuous random variables.
    The GKOV estimator is defined as:
    \begin{equation}
        I_n(X;Y) = \frac{1}{n} \sum_{i=1}^n \hat{I}_i = \sum_{i=1}^n \left( \psi(\tilde{t}_i) + \log n - \log(n_{x,i}+1) - \log(n_{y,i}+1) \right)\label{eq:gkov}
    \end{equation}
    where $\psi$ is the digamma function.

    \subsection{Motivation}\label{subsec:motivation}
    As previou

    \section{Project Execution}\label{sec:execution}
    The project aimed to develop a C++ API for the GKOV estimator, the histogram estimator, and a simulator for the leakage of a device.
    All the code was written in C++ and using an OO approach.

    \subsection{GKOV estimator}\label{subsec:gkov_impl}
    The GKOV estimator was implemented as described in~\cite{gkov}.
    The implementation is based on the class \texttt{GKOVEstimator} which is initialized with the callback function to compute the value $t_n$ as described in~\cite{roy_leakage_2022}.
    The class \texttt{GKOVEstimator} provides the method \texttt{estimate} which takes as input the discrete variable $X$, the continuous variable $Y$, and the dimension of the two variables.
    The method \texttt{estimate} returns the estimated MI between $X$ and $Y$.
    Under the hood, the method \texttt{estimate} computes the value $t$, builds the matrix data starting from the discrete and continuous variables, builds the search trees for the two single variables and the combined matrix, and finally computes the GKOV estimator as described in~\cite{roy_leakage_2022}.

    \subsection{Histogram estimator}\label{subsec:hist_impl}
    The histogram estimator was implemented to be used as a baseline for comparing the GKOV estimator.
    Following the description in Section~\ref{subsec:hist}, the histogram estimator was implemented as a class \texttt{HistogramEstimator}.
    The class \texttt{HistogramEstimator} is initialized with the number of dimensions of the continuous variable $Y$, the number of bins for each dimension, and the ranges of the bins.
    The underlying assumption about the ranges is that the bins are equally spaced.
    The class \texttt{HistogramEstimator} provides the method \texttt{estimate} which takes as input the discrete variable $X$, its probability distribution, the continuous variable $Y$, the size of the two variables and the dimension of the $Y$ variable.

    The method \texttt{estimate} makes usage of the methods \texttt{build\_histogram}, \texttt{pdf\_entropy}, and \texttt{conditional\_entropy} to compute the MI between $X$ and $Y$.
    \texttt{build\_histogram} builds histogram of $Y$ based on the dimensions of the variable and computes the pdf of the histogram.
    To ensure the efficiency of the histogram estimator, the method makes usage of the GSL library~\cite{gsl} for building the 1D and 2D histograms.
    The method \texttt{pdf\_entropy} computes the entropy of the pdf of $Y$.
    The method \texttt{conditional\_entropy} computes the conditional entropy of $Y$ given $X$.
    Finally, the method \texttt{estimate} returns the MI between $X$ and $Y$ computed as described in equation~\ref{eq:hist}.

    \subsection{Simulator}\label{subsec:sim}
    The simulator was implemented to provide a way to test the GKOV estimator and the histogram estimator.
    This class has the purpose of simulating the leakage of a device using the following model:
    \begin{equation}
        \text{leakage} = \text{leakage\_function}(\text{crypto\_function}(key, \text{plaintext})) + \text{noise}\label{eq:sim}
    \end{equation}
    where \texttt{leakage\_function} is the function that simulates the leakage of the device, \texttt{crypto\_function} is the function that simulates the cryptographic function of the device, and \texttt{noise} is the noise added to the leakage.
    At time of writing, the simulator supports gaussian and laplacian noise distributions but the code was made to be easily extensible to other distributions.
    Instead, the cryptographic function is passed as argument of the methods of the simulator.

    Regarding the type of trace that can be simulated, the simulator supports one dimensional traces.

    Finally, the simulator saves the simulated traces in a HDF5 file to allow the user to use the traces for further analysis.

    \subsection{Utilities}\label{subsec:utils}
    The project provides a class that implements some utilities used by the other classes in the project.

    In particular, the class \texttt{Utils} provides the methods:
    \begin{itemize}
        \item \texttt{flatten} and \texttt{to\_gkov\_format} to convert an n-dimensional array to, respectively, a 1-dimensional array and a 2-dimensional array;
        \item \texttt{compute\_distribution} to compute the distribution of a discrete variable;
        \item \texttt{read\_traces} and \texttt{write\_traces} to read and write traces from and to a HDF5 file.
    \end{itemize}
    \newpage
    \bibliography{references}
    \bibliographystyle{acm}
\end{document}



